{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18c529fb-745c-45cc-a293-6be508b53418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载词向量，这可能需要几分钟，请耐心等待...\n",
      "加载完成！\n",
      "'广东' 的向量主要维度前5位: [-0.126287 -0.039829  0.131457 -0.100277  0.117989]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 路径替换为你下载的txt文件路径\n",
    "file_path = 'tencent-ailab-embedding-zh-d200-v0.2.0.txt'\n",
    "\n",
    "print(\"正在加载词向量，这可能需要几分钟，请耐心等待...\")\n",
    "\n",
    "# limit参数：如果你内存不够（比如只有8G/16G），建议加上 limit=500000\n",
    "# 这表示只加载最常用的前50万个词，通常足够覆盖省份和常用形容词\n",
    "# 如果内存充足（32G以上），可以去掉 limit 参数加载全部\n",
    "wv = KeyedVectors.load_word2vec_format(file_path, binary=False, limit=5000000)\n",
    "\n",
    "print(\"加载完成！\")\n",
    "\n",
    "# 测试一下是否成功\n",
    "test_word = '广东'\n",
    "if test_word in wv:\n",
    "    print(f\"'{test_word}' 的向量主要维度前5位: {wv[test_word][:5]}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' 不在词表中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5f4dc-4ca4-401f-a8a8-7387f7811f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "province_definitions = {\n",
    "    # --- 直辖市 ---\n",
    "    '北京': ['北京', '京', '北京人'],\n",
    "    '上海': ['上海', '沪', '上海人'],\n",
    "    '天津': ['天津', '津', '天津人'],\n",
    "    '重庆': ['重庆', '渝', '重庆人'],\n",
    "\n",
    "    # --- 华北地区 ---\n",
    "    '河北': ['河北', '冀', '河北人'],\n",
    "    '山西': ['山西', '晋', '山西人'],\n",
    "    '内蒙古': ['内蒙古', '蒙', '内蒙', '内蒙古人'], # \"蒙\"有时会有歧义，但在语境中通常可用\n",
    "\n",
    "    # --- 东北地区 (既包含单个省，也包含\"东北\"整体概念) ---\n",
    "    '东北': ['东北', '东北人'], # 作为一个文化整体\n",
    "    '辽宁': ['辽宁', '辽', '辽宁人'],\n",
    "    '吉林': ['吉林', '吉', '吉林人'], # 注意：\"吉\"字可能有\"吉利\"的噪音，但在平均算法下可控\n",
    "    '黑龙江': ['黑龙江', '黑', '黑龙江人'],\n",
    "\n",
    "    # --- 华东地区 ---\n",
    "    '江苏': ['江苏', '苏', '江苏人'],\n",
    "    '浙江': ['浙江', '浙', '浙江人'],\n",
    "    '安徽': ['安徽', '皖', '安徽人'],\n",
    "    '福建': ['福建', '闽', '福建人'],\n",
    "    '江西': ['江西', '赣', '江西人'],\n",
    "    '山东': ['山东', '鲁', '山东人'],\n",
    "    # 台湾在地理分区上常归入华东或单独列出\n",
    "    '台湾': ['台湾', '台', '台湾人'],\n",
    "\n",
    "    # --- 华中地区 ---\n",
    "    '河南': ['河南', '豫', '河南人'],\n",
    "    '湖北': ['湖北', '鄂', '湖北人'],\n",
    "    '湖南': ['湖南', '湘', '湖南人'],\n",
    "\n",
    "    # --- 华南地区 ---\n",
    "    '广东': ['广东', '粤', '广东人'],\n",
    "    '广西': ['广西', '桂', '广西人'],\n",
    "    '海南': ['海南', '琼', '海南人'],\n",
    "    '香港': ['香港', '港', '香港人'],\n",
    "    '澳门': ['澳门', '澳', '澳门人'],\n",
    "\n",
    "    # --- 西南地区 ---\n",
    "    '四川': ['四川', '川', '蜀', '四川人'], # 包含双简称\n",
    "    '贵州': ['贵州', '贵', '黔', '贵州人'], # 包含双简称\n",
    "    '云南': ['云南', '云', '滇', '云南人'], # 包含双简称\n",
    "    '西藏': ['西藏', '藏', '西藏人'],\n",
    "\n",
    "    # --- 西北地区 ---\n",
    "    '陕西': ['陕西', '陕', '秦', '陕西人'], # 包含双简称\n",
    "    '甘肃': ['甘肃', '甘', '陇', '甘肃人'], # 包含双简称\n",
    "    '青海': ['青海', '青', '青海人'],\n",
    "    '宁夏': ['宁夏', '宁', '宁夏人'],\n",
    "    '新疆': ['新疆', '新', '新疆人'] # \"新\"字可能有噪音，但\"新疆人\"很准\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 辅助检查代码：打印一下看看有多少个区域\n",
    "print(f\"共定义了 {len(province_definitions)} 个区域/省份。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284da9e-2e6f-44b1-9a25-d872f952a0b4",
   "metadata": {},
   "source": [
    "### 1.计算不同省份 和 词汇的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "448a43a9-484b-4335-9d08-1c41a978c55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词表大小: 11310\n",
      "在词向量中找到的有效词汇数量: 11302\n",
      "正在计算相似度矩阵...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 11302/11302 [00:50<00:00, 225.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "计算完成！前 5 行预览：\n",
      "          北京      上海      天津      重庆      河北      山西     内蒙古      东北      辽宁  \\\n",
      "word                                                                           \n",
      "阿谀奉承  0.1784  0.1455  0.1921  0.1981  0.1779  0.2280  0.1843  0.2521  0.1786   \n",
      "哀号    0.1150  0.1257  0.1241  0.1572  0.1610  0.1581  0.1798  0.1462  0.1438   \n",
      "哀求    0.1388  0.1129  0.1607  0.1592  0.1913  0.2034  0.2055  0.1531  0.1744   \n",
      "哀伤    0.1447  0.1331  0.1642  0.1685  0.1931  0.2042  0.2109  0.1723  0.1862   \n",
      "哀痛    0.1063  0.0989  0.1174  0.1634  0.1899  0.1960  0.1753  0.1152  0.1748   \n",
      "\n",
      "          吉林  ...      澳门      四川      贵州      云南      西藏      陕西      甘肃  \\\n",
      "word          ...                                                           \n",
      "阿谀奉承  0.1572  ...  0.1244  0.2110  0.1879  0.2031  0.1911  0.2030  0.1824   \n",
      "哀号    0.1447  ...  0.0807  0.1951  0.1440  0.2070  0.1861  0.1749  0.2227   \n",
      "哀求    0.1724  ...  0.1223  0.2035  0.1637  0.1957  0.1564  0.2421  0.2257   \n",
      "哀伤    0.2173  ...  0.1549  0.2349  0.1683  0.2561  0.2452  0.2250  0.2584   \n",
      "哀痛    0.1730  ...  0.1236  0.1969  0.1545  0.2114  0.1979  0.2044  0.2292   \n",
      "\n",
      "          青海      宁夏      新疆  \n",
      "word                          \n",
      "阿谀奉承  0.1704  0.1582  0.1429  \n",
      "哀号    0.1898  0.1773  0.1429  \n",
      "哀求    0.2147  0.2129  0.1263  \n",
      "哀伤    0.2525  0.2186  0.1774  \n",
      "哀痛    0.2030  0.1699  0.1173  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "结果已保存至: province_similarity_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "\n",
    "\n",
    "csv_path = '13428_2021_1607_MOESM1_ESM.csv'  # 你的文件名\n",
    "try:\n",
    "    df_words = pd.read_csv(csv_path)\n",
    "    # 如果 CSV 没有表头，可以用 header=None，并指定列名\n",
    "    # df_words = pd.read_csv(csv_path, header=None, names=['word'])\n",
    "    \n",
    "    # 确保主要是字符串，去除空值\n",
    "    word_list = df_words['Word'].dropna().astype(str).tolist()\n",
    "    print(f\"原始词表大小: {len(word_list)}\")\n",
    "except Exception as e:\n",
    "    print(f\"读取CSV失败: {e}\")\n",
    "    # 用于测试的假数据\n",
    "    word_list = ['井盖', '面食', '大葱', '细腻', '火锅', '挖掘机', '豪爽', '诈骗']\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 定义省份同义词组 (Synonym Dictionary)\n",
    "# ==============================================================================\n",
    "# 这种方法能让结果更鲁棒（Robust）\n",
    "province_definitions = {\n",
    "    # --- 直辖市 ---\n",
    "    '北京': ['北京', '京', '北京人'],\n",
    "    '上海': ['上海', '沪', '上海人'],\n",
    "    '天津': ['天津', '津', '天津人'],\n",
    "    '重庆': ['重庆', '渝', '重庆人'],\n",
    "\n",
    "    # --- 华北地区 ---\n",
    "    '河北': ['河北', '冀', '河北人'],\n",
    "    '山西': ['山西', '晋', '山西人'],\n",
    "    '内蒙古': ['内蒙古', '蒙', '内蒙', '内蒙古人'], # \"蒙\"有时会有歧义，但在语境中通常可用\n",
    "\n",
    "    # --- 东北地区 (既包含单个省，也包含\"东北\"整体概念) ---\n",
    "    '东北': ['东北', '东北人'], # 作为一个文化整体\n",
    "    '辽宁': ['辽宁', '辽', '辽宁人'],\n",
    "    '吉林': ['吉林', '吉', '吉林人'], # 注意：\"吉\"字可能有\"吉利\"的噪音，但在平均算法下可控\n",
    "    '黑龙江': ['黑龙江', '黑', '黑龙江人'],\n",
    "\n",
    "    # --- 华东地区 ---\n",
    "    '江苏': ['江苏', '苏', '江苏人'],\n",
    "    '浙江': ['浙江', '浙', '浙江人'],\n",
    "    '安徽': ['安徽', '皖', '安徽人'],\n",
    "    '福建': ['福建', '闽', '福建人'],\n",
    "    '江西': ['江西', '赣', '江西人'],\n",
    "    '山东': ['山东', '鲁', '山东人'],\n",
    "    # 台湾在地理分区上常归入华东或单独列出\n",
    "    '台湾': ['台湾', '台', '台湾人'],\n",
    "\n",
    "    # --- 华中地区 ---\n",
    "    '河南': ['河南', '豫', '河南人'],\n",
    "    '湖北': ['湖北', '鄂', '湖北人'],\n",
    "    '湖南': ['湖南', '湘', '湖南人'],\n",
    "\n",
    "    # --- 华南地区 ---\n",
    "    '广东': ['广东', '粤', '广东人'],\n",
    "    '广西': ['广西', '桂', '广西人'],\n",
    "    '海南': ['海南', '琼', '海南人'],\n",
    "    '香港': ['香港', '港', '香港人'],\n",
    "    '澳门': ['澳门', '澳', '澳门人'],\n",
    "\n",
    "    # --- 西南地区 ---\n",
    "    '四川': ['四川', '川', '蜀', '四川人'], # 包含双简称\n",
    "    '贵州': ['贵州', '贵', '黔', '贵州人'], # 包含双简称\n",
    "    '云南': ['云南', '云', '滇', '云南人'], # 包含双简称\n",
    "    '西藏': ['西藏', '藏', '西藏人'],\n",
    "\n",
    "    # --- 西北地区 ---\n",
    "    '陕西': ['陕西', '陕', '秦', '陕西人'], # 包含双简称\n",
    "    '甘肃': ['甘肃', '甘', '陇', '甘肃人'], # 包含双简称\n",
    "    '青海': ['青海', '青', '青海人'],\n",
    "    '宁夏': ['宁夏', '宁', '宁夏人'],\n",
    "    '新疆': ['新疆', '新', '新疆人'] # \"新\"字可能有噪音，但\"新疆人\"很准\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 核心计算函数\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_group_similarity(target_word, group_keywords, model):\n",
    "    \"\"\"\n",
    "    计算 target_word 和 group_keywords 中所有词的相似度，然后取平均。\n",
    "    实现你要求的：{‘豫’，“河南”，“河南人”} 分别计算 cos 相似度，再平均。\n",
    "    \"\"\"\n",
    "    if target_word not in model:\n",
    "        return None  # 如果常用词本身不在模型里，跳过\n",
    "    \n",
    "    scores = []\n",
    "    for key in group_keywords:\n",
    "        if key in model:\n",
    "            # 计算余弦相似度\n",
    "            sim = model.similarity(target_word, key)\n",
    "            scores.append(sim)\n",
    "    \n",
    "    # 如果这个省份的所有关键词都不在模型里（极少见），返回 None\n",
    "    if not scores:\n",
    "        return None\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. 执行批量计算\n",
    "# ==============================================================================\n",
    "\n",
    "# 过滤：先确保常用的词都在模型里，减少循环中的判断\n",
    "valid_words = [w for w in word_list if w in wv]\n",
    "print(f\"在词向量中找到的有效词汇数量: {len(valid_words)}\")\n",
    "\n",
    "# 用于存储结果的字典列表\n",
    "results = []\n",
    "\n",
    "print(\"正在计算相似度矩阵...\")\n",
    "# 使用 tqdm 显示进度条\n",
    "for word in tqdm(valid_words):\n",
    "    row_data = {'word': word}\n",
    "    \n",
    "    for prov_name, keywords in province_definitions.items():\n",
    "        # 调用上面的函数计算平均相似度\n",
    "        avg_sim = calculate_group_similarity(word, keywords, wv)\n",
    "        \n",
    "        # 存入数据，保留4位小数\n",
    "        if avg_sim is not None:\n",
    "            row_data[prov_name] = round(float(avg_sim), 4)\n",
    "        else:\n",
    "            row_data[prov_name] = np.nan # 或者 0\n",
    "            \n",
    "    results.append(row_data)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 生成表格并保存\n",
    "# ==============================================================================\n",
    "\n",
    "# 转换为 DataFrame\n",
    "df_result = pd.DataFrame(results)\n",
    "\n",
    "# 设置 'word' 为索引，这样表格看起来就是：行=词，列=省份\n",
    "df_result.set_index('word', inplace=True)\n",
    "\n",
    "print(\"\\n计算完成！前 5 行预览：\")\n",
    "print(df_result.head())\n",
    "\n",
    "# 保存结果\n",
    "output_file = 'province_similarity_matrix.csv'\n",
    "df_result.to_csv(output_file, encoding='utf_8_sig') # sig 防止中文乱码\n",
    "print(f\"\\n结果已保存至: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784211e7-ce0f-4b91-b8ba-e7f7262cb617",
   "metadata": {},
   "source": [
    "### 找出省份最相似traits\n",
    ">计算每个省份最相似的10个traits词汇。再分正面负面找 10个最相似traits 词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e241b338-95fb-4faf-8454-0890f9dd2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence = pd.read_csv('13428_2021_1607_MOESM1_ESM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "823828c5-e182-4c05-9da7-28dd26344415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Word_ID', 'Word', 'Valence_Mean', 'Valence_SD', 'Valence_N',\n",
       "       'Arousal_Mean', 'Arousal_SD', 'Arousal_N', 'Men_Valence_Mean',\n",
       "       'Men_Valence_SD', 'Men_Valence_N', 'Women_Valence_Mean',\n",
       "       'Women_Valence_SD', 'Women_Valence_N', 'Men_Arousal_Mean',\n",
       "       'Men_Arousal_SD', 'Men_Arousal_N', 'Women_Arousal_Mean',\n",
       "       'Women_Arousal_SD', 'Women_Arousal_N'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa422817-efa8-4a01-bd35-8be5f6bdf4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence = valence[['Word', 'Valence_Mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e09cd6ce-8aea-4019-ad1b-cf95551937e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>阿谀奉承</td>\n",
       "      <td>-2.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>哀号</td>\n",
       "      <td>-1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>哀求</td>\n",
       "      <td>-1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>哀伤</td>\n",
       "      <td>-1.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>哀痛</td>\n",
       "      <td>-1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11305</th>\n",
       "      <td>座右铭</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11306</th>\n",
       "      <td>做东</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11307</th>\n",
       "      <td>做梦</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>做主</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>做作</td>\n",
       "      <td>-1.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11310 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Valence_Mean\n",
       "0      阿谀奉承         -2.15\n",
       "1        哀号         -1.84\n",
       "2        哀求         -1.69\n",
       "3        哀伤         -1.85\n",
       "4        哀痛         -1.75\n",
       "...     ...           ...\n",
       "11305   座右铭          1.28\n",
       "11306    做东          0.83\n",
       "11307    做梦          0.15\n",
       "11308    做主          0.77\n",
       "11309    做作         -1.36\n",
       "\n",
       "[11310 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f15fd36-1054-4b14-955b-cc092ea228d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分析完成！已生成三个文件：总体(含均值)、正向Top10、负向Top10。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ================= 1. 准备 Valence 数据 =================\n",
    "# 假设 valence 数据文件名为 'valence.csv'，包含 'Word' 和 'Valence_Mean' 列\n",
    "# 制作字典映射：{'词汇': 分数}，查找速度最快\n",
    "val_df = valence.copy()\n",
    "valence_map = val_df.set_index('Word')['Valence_Mean'].to_dict()\n",
    "\n",
    "# ================= 2. 读取矩阵并预处理 =================\n",
    "# 读取列表和矩阵\n",
    "with open(\"final_trait_list.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    final_list = f.read().splitlines()\n",
    "\n",
    "# 读取相似度矩阵\n",
    "df = pd.read_csv('province_similarity_matrix.csv', encoding='utf-8-sig', index_col=0)\n",
    "\n",
    "# 【关键步骤】\n",
    "# 1. 筛选行：必须在 trait 列表中，且必须在 wv 模型中（你的代码中有这一步）\n",
    "# 2. 映射 Valence：直接给矩阵加一列 'valence'，方便后续筛选\n",
    "valid_traits = [w for w in final_list if w in wv] # 确保 wv 在上下文中存在\n",
    "df = df[df.index.isin(valid_traits)].copy()       # 过滤行\n",
    "df['valence'] = df.index.map(valence_map)         # 匹配效价分数\n",
    "\n",
    "# 去除没有 valence 数据的词（可选，为了计算平均值准确）\n",
    "df = df.dropna(subset=['valence']) \n",
    "\n",
    "# ================= 3. 核心计算函数 =================\n",
    "def analyze_province(col_series):\n",
    "    \"\"\"\n",
    "    对每个省份的一列数据进行分析\n",
    "    col_series.name 是省份名，col_series.index 是词汇\n",
    "    注意：此时 col_series 包含 'valence' 列，需要排除\n",
    "    \"\"\"\n",
    "    # 获取该列对应的词汇相似度（排除 valence 列）\n",
    "    sim_scores = col_series\n",
    "    \n",
    "    # --- A. 总体 Top 10 及其平均 Valence ---\n",
    "    top10_general = sim_scores.nlargest(10).index.tolist()\n",
    "    # 通过词汇列表查所有的 valence 并求平均\n",
    "    avg_valence = df.loc[top10_general, 'valence'].mean()\n",
    "    \n",
    "    # --- B. 正向 Top 10 (Valence > 0) ---\n",
    "    # 利用 df['valence'] 进行布尔筛选，再找该省相似度最高的\n",
    "    pos_words = sim_scores[df['valence'] > 0].nlargest(10).index.tolist()\n",
    "    \n",
    "    # --- C. 负向 Top 10 (Valence < 0) ---\n",
    "    neg_words = sim_scores[df['valence'] < 0].nlargest(10).index.tolist()\n",
    "    \n",
    "    return top10_general, avg_valence, pos_words, neg_words\n",
    "\n",
    "# ================= 4. 执行并保存 =================\n",
    "results_general = {}\n",
    "results_avg_val = {}\n",
    "results_pos = {}\n",
    "results_neg = {}\n",
    "\n",
    "# 遍历省份列（排除最后一列 'valence'）\n",
    "province_cols = [c for c in df.columns if c != 'valence']\n",
    "\n",
    "for province in province_cols:\n",
    "    t10, avg, pos10, neg10 = analyze_province(df[province])\n",
    "    \n",
    "    results_general[province] = t10\n",
    "    results_avg_val[province] = avg\n",
    "    results_pos[province] = pos10\n",
    "    results_neg[province] = neg10\n",
    "\n",
    "# --- 转换格式并保存 ---\n",
    "\n",
    "# 1. 总体 Top 10 表 (并在最后一行追加平均 Valence)\n",
    "df_res_general = pd.DataFrame(results_general)\n",
    "# 追加一行平均值\n",
    "df_res_general.loc['Mean_Valence'] = pd.Series(results_avg_val)\n",
    "df_res_general.to_csv(\"result_top10_general_with_mean.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2. 正向 Top 10 表\n",
    "pd.DataFrame(results_pos).to_csv(\"result_top10_positive.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 3. 负向 Top 10 表\n",
    "pd.DataFrame(results_neg).to_csv(\"result_top10_negative.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"分析完成！已生成三个文件：总体(含均值)、正向Top10、负向Top10。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2778a91-34cf-4cec-8f08-9a46c0b26a59",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>北京</th>\n",
       "      <th>上海</th>\n",
       "      <th>天津</th>\n",
       "      <th>重庆</th>\n",
       "      <th>河北</th>\n",
       "      <th>山西</th>\n",
       "      <th>内蒙古</th>\n",
       "      <th>东北</th>\n",
       "      <th>辽宁</th>\n",
       "      <th>吉林</th>\n",
       "      <th>...</th>\n",
       "      <th>澳门</th>\n",
       "      <th>四川</th>\n",
       "      <th>贵州</th>\n",
       "      <th>云南</th>\n",
       "      <th>西藏</th>\n",
       "      <th>陕西</th>\n",
       "      <th>甘肃</th>\n",
       "      <th>青海</th>\n",
       "      <th>宁夏</th>\n",
       "      <th>新疆</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>可靠</td>\n",
       "      <td>歧视</td>\n",
       "      <td>纯正</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>本分</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>好客</td>\n",
       "      <td>...</td>\n",
       "      <td>乐天</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>好客</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>淳朴</td>\n",
       "      <td>张扬</td>\n",
       "      <td>寒微</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>纯正</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>卓越</td>\n",
       "      <td>卓越</td>\n",
       "      <td>老实</td>\n",
       "      <td>好客</td>\n",
       "      <td>歧视</td>\n",
       "      <td>骁勇</td>\n",
       "      <td>纯正</td>\n",
       "      <td>好客</td>\n",
       "      <td>歧视</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>...</td>\n",
       "      <td>歧视</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>逍遥</td>\n",
       "      <td>纯朴</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>好客</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>野蛮</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>纯正</td>\n",
       "      <td>讨厌</td>\n",
       "      <td>仁爱</td>\n",
       "      <td>仁爱</td>\n",
       "      <td>志愿</td>\n",
       "      <td>本分</td>\n",
       "      <td>好客</td>\n",
       "      <td>豪放</td>\n",
       "      <td>专一</td>\n",
       "      <td>纯正</td>\n",
       "      <td>...</td>\n",
       "      <td>卓越</td>\n",
       "      <td>好客</td>\n",
       "      <td>小气</td>\n",
       "      <td>好客</td>\n",
       "      <td>虔诚</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>张扬</td>\n",
       "      <td>淳朴</td>\n",
       "      <td>卓越</td>\n",
       "      <td>卓越</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>诚信</td>\n",
       "      <td>诚信</td>\n",
       "      <td>博爱</td>\n",
       "      <td>张扬</td>\n",
       "      <td>老实</td>\n",
       "      <td>耿直</td>\n",
       "      <td>歧视</td>\n",
       "      <td>歧视</td>\n",
       "      <td>志愿</td>\n",
       "      <td>文静</td>\n",
       "      <td>...</td>\n",
       "      <td>纯正</td>\n",
       "      <td>仁义</td>\n",
       "      <td>奢侈</td>\n",
       "      <td>张扬</td>\n",
       "      <td>好客</td>\n",
       "      <td>老实</td>\n",
       "      <td>骁勇</td>\n",
       "      <td>逍遥</td>\n",
       "      <td>文静</td>\n",
       "      <td>独立</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>老实</td>\n",
       "      <td>势利</td>\n",
       "      <td>口才</td>\n",
       "      <td>健谈</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>老实</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>骁勇</td>\n",
       "      <td>诚实</td>\n",
       "      <td>...</td>\n",
       "      <td>友好</td>\n",
       "      <td>老实</td>\n",
       "      <td>黑心</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>慈悲</td>\n",
       "      <td>骁勇</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>仁爱</td>\n",
       "      <td>歧视</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>歧视</td>\n",
       "      <td>博爱</td>\n",
       "      <td>乐天</td>\n",
       "      <td>吹牛</td>\n",
       "      <td>仁爱</td>\n",
       "      <td>牛脾气</td>\n",
       "      <td>淳朴</td>\n",
       "      <td>淳朴</td>\n",
       "      <td>友好</td>\n",
       "      <td>老实</td>\n",
       "      <td>...</td>\n",
       "      <td>诚信</td>\n",
       "      <td>耿直</td>\n",
       "      <td>纯正</td>\n",
       "      <td>文静</td>\n",
       "      <td>豪放</td>\n",
       "      <td>仁义</td>\n",
       "      <td>老实</td>\n",
       "      <td>纯正</td>\n",
       "      <td>好客</td>\n",
       "      <td>好客</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>乐天</td>\n",
       "      <td>小气</td>\n",
       "      <td>吹牛</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>能干</td>\n",
       "      <td>忠烈</td>\n",
       "      <td>豪放</td>\n",
       "      <td>老实</td>\n",
       "      <td>本分</td>\n",
       "      <td>友好</td>\n",
       "      <td>...</td>\n",
       "      <td>信誉</td>\n",
       "      <td>豪放</td>\n",
       "      <td>淳朴</td>\n",
       "      <td>淳朴</td>\n",
       "      <td>豪爽</td>\n",
       "      <td>文静</td>\n",
       "      <td>志气</td>\n",
       "      <td>纯朴</td>\n",
       "      <td>博爱</td>\n",
       "      <td>淳朴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>万恶</td>\n",
       "      <td>万恶</td>\n",
       "      <td>可靠</td>\n",
       "      <td>博爱</td>\n",
       "      <td>骁勇</td>\n",
       "      <td>博学</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>耿直</td>\n",
       "      <td>文静</td>\n",
       "      <td>本分</td>\n",
       "      <td>...</td>\n",
       "      <td>怀旧</td>\n",
       "      <td>忠义</td>\n",
       "      <td>可靠</td>\n",
       "      <td>爽快</td>\n",
       "      <td>奔放</td>\n",
       "      <td>混账</td>\n",
       "      <td>志向</td>\n",
       "      <td>张扬</td>\n",
       "      <td>老实</td>\n",
       "      <td>老实</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>信誉</td>\n",
       "      <td>挑剔</td>\n",
       "      <td>歧视</td>\n",
       "      <td>老实</td>\n",
       "      <td>刁民</td>\n",
       "      <td>占便宜</td>\n",
       "      <td>志愿</td>\n",
       "      <td>吹牛</td>\n",
       "      <td>豪迈</td>\n",
       "      <td>能干</td>\n",
       "      <td>...</td>\n",
       "      <td>好客</td>\n",
       "      <td>专一</td>\n",
       "      <td>老实</td>\n",
       "      <td>老实</td>\n",
       "      <td>纯正</td>\n",
       "      <td>英明</td>\n",
       "      <td>豪放</td>\n",
       "      <td>文静</td>\n",
       "      <td>纯正</td>\n",
       "      <td>能干</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>鄙视</td>\n",
       "      <td>纯正</td>\n",
       "      <td>友好</td>\n",
       "      <td>文静</td>\n",
       "      <td>专一</td>\n",
       "      <td>渊博</td>\n",
       "      <td>野蛮</td>\n",
       "      <td>纯正</td>\n",
       "      <td>博学</td>\n",
       "      <td>歧视</td>\n",
       "      <td>...</td>\n",
       "      <td>拜金</td>\n",
       "      <td>逍遥</td>\n",
       "      <td>好学</td>\n",
       "      <td>君子</td>\n",
       "      <td>纯洁</td>\n",
       "      <td>流氓</td>\n",
       "      <td>安分</td>\n",
       "      <td>豪放</td>\n",
       "      <td>混账</td>\n",
       "      <td>优秀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_Valence</th>\n",
       "      <td>0.552</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.841</td>\n",
       "      <td>1.048</td>\n",
       "      <td>0.808</td>\n",
       "      <td>1.139</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.879</td>\n",
       "      <td>1.232</td>\n",
       "      <td>1.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.555</td>\n",
       "      <td>0.724</td>\n",
       "      <td>1.217</td>\n",
       "      <td>1.629</td>\n",
       "      <td>0.634</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 北京     上海     天津     重庆     河北     山西    内蒙古     东北     辽宁  \\\n",
       "0                可靠     歧视     纯正     豪爽     本分     豪爽     豪爽     豪爽     豪爽   \n",
       "1                卓越     卓越     老实     好客     歧视     骁勇     纯正     好客     歧视   \n",
       "2                纯正     讨厌     仁爱     仁爱     志愿     本分     好客     豪放     专一   \n",
       "3                诚信     诚信     博爱     张扬     老实     耿直     歧视     歧视     志愿   \n",
       "4                老实     势利     口才     健谈     豪爽     豪迈     老实     豪迈     骁勇   \n",
       "5                歧视     博爱     乐天     吹牛     仁爱    牛脾气     淳朴     淳朴     友好   \n",
       "6                乐天     小气     吹牛     豪迈     能干     忠烈     豪放     老实     本分   \n",
       "7                万恶     万恶     可靠     博爱     骁勇     博学     豪迈     耿直     文静   \n",
       "8                信誉     挑剔     歧视     老实     刁民    占便宜     志愿     吹牛     豪迈   \n",
       "9                鄙视     纯正     友好     文静     专一     渊博     野蛮     纯正     博学   \n",
       "Mean_Valence  0.552 -0.276  0.841  1.048  0.808  1.139  0.832  0.879  1.232   \n",
       "\n",
       "               吉林  ...     澳门     四川     贵州     云南     西藏     陕西     甘肃    青海  \\\n",
       "0              好客  ...     乐天     豪爽     好客     豪爽     淳朴     张扬     寒微    豪爽   \n",
       "1              豪爽  ...     歧视     豪迈     豪爽     逍遥     纯朴     豪爽     豪爽    好客   \n",
       "2              纯正  ...     卓越     好客     小气     好客     虔诚     豪迈     张扬    淳朴   \n",
       "3              文静  ...     纯正     仁义     奢侈     张扬     好客     老实     骁勇    逍遥   \n",
       "4              诚实  ...     友好     老实     黑心     豪迈     慈悲     骁勇     豪迈    豪迈   \n",
       "5              老实  ...     诚信     耿直     纯正     文静     豪放     仁义     老实    纯正   \n",
       "6              友好  ...     信誉     豪放     淳朴     淳朴     豪爽     文静     志气    纯朴   \n",
       "7              本分  ...     怀旧     忠义     可靠     爽快     奔放     混账     志向    张扬   \n",
       "8              能干  ...     好客     专一     老实     老实     纯正     英明     豪放    文静   \n",
       "9              歧视  ...     拜金     逍遥     好学     君子     纯洁     流氓     安分    豪放   \n",
       "Mean_Valence  1.2  ...  0.955  1.555  0.724  1.217  1.629  0.634  1.064  1.29   \n",
       "\n",
       "                宁夏     新疆  \n",
       "0               豪爽     纯正  \n",
       "1               豪迈     野蛮  \n",
       "2               卓越     卓越  \n",
       "3               文静     独立  \n",
       "4               仁爱     歧视  \n",
       "5               好客     好客  \n",
       "6               博爱     淳朴  \n",
       "7               老实     老实  \n",
       "8               纯正     能干  \n",
       "9               混账     优秀  \n",
       "Mean_Valence  1.22  0.998  \n",
       "\n",
       "[11 rows x 35 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res_general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28037871-5c24-4817-9a8d-bf7a9b702dbc",
   "metadata": {},
   "source": [
    "### 3.计算省份间两两对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8bc0d1-6e57-4e13-8961-423bf0c48560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终参与计算的有效词汇数: 799\n",
      "   Group A vs. Group B                       Top 10 Relative Traits  \\\n",
      "40      上海          东北       卓越, 健忘, 周到, 准时, 细致, 审慎, 锐意, 拖延, 可靠, 完美   \n",
      "61      上海          云南    泼妇, 诚信, 倚老卖老, 素质, 势利, 怀旧, 人情味, 歧视, 拜金, 外向   \n",
      "39      上海         内蒙古  拜金, 人情味, 从良, 倚老卖老, 势利, 刻薄, 泼妇, 乐天, 挑剔, 八面玲珑   \n",
      "34      上海          北京     念旧, 激进, 泼妇, 拜金, 同情心, 陋习, 风骚, 痴心, 求知欲, 怪僻   \n",
      "50      上海          台湾     挑剔, 急躁, 恒心, 细心, 淘气, 泼妇, 机智, 絮叨, 洒脱, 体贴入微   \n",
      "\n",
      "    Average Valence  \n",
      "40            0.955  \n",
      "61           -0.249  \n",
      "39           -0.571  \n",
      "34           -0.443  \n",
      "50            0.277  \n",
      "计算完成，结果已保存至 province_pairwise_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "\n",
    "# ================= 1. 数据准备与对齐 =================\n",
    "\n",
    "# 1.1 读取 Valence 数据，建立查找表\n",
    "val_df = pd.read_csv('13428_2021_1607_MOESM1_ESM.csv')[['Word', 'Valence_Mean']]\n",
    "# 假设列名是 'Word' 和 'Valence_Mean'\n",
    "valence_map = val_df.set_index('Word')['Valence_Mean'].to_dict()\n",
    "\n",
    "# 1.2 读取相似度矩阵\n",
    "df_sim = pd.read_csv('province_similarity_matrix.csv', encoding='utf-8-sig', index_col=0)\n",
    "\n",
    "# 1.3 读取你的 trait 列表\n",
    "with open(\"final_trait_list.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    final_list = f.read().splitlines()\n",
    "\n",
    "# 1.4 【关键过滤】求交集：必须在 trait列表里，且在 valence 库里有分数的词\n",
    "# 注意：这里假设 df_sim 里的词已经在 wv 里检查过了，如果没检查，这里加上 if w in wv\n",
    "valid_words = final_list\n",
    "\n",
    "# 1.5 重构矩阵：只保留有效词，并新增一列 valence 用于后续计算\n",
    "df_clean = df_sim.loc[valid_words].copy()\n",
    "df_clean['valence_score'] = df_clean.index.map(valence_map)\n",
    "\n",
    "print(f\"最终参与计算的有效词汇数: {len(df_clean)}\")\n",
    "\n",
    "# ================= 2. 两两对比计算 (Pairwise Comparison) =================\n",
    "\n",
    "results = []\n",
    "provinces = [col for col in df_clean.columns if col != 'valence_score']\n",
    "\n",
    "# 使用 permutations 生成排列：(北京, 上海) 和 (上海, 北京) 都会生成\n",
    "for p_a, p_b in permutations(provinces, 2):\n",
    "    \n",
    "    # --- 核心算法：相对相似度 = A的相似度 - B的相似度 ---\n",
    "    # 直接整列相减，Pandas 会自动对齐索引\n",
    "    diff_series = df_clean[p_a] - df_clean[p_b]\n",
    "    \n",
    "    # 找出差值最大的 Top 10 (即 A 相比 B 最突出的词)\n",
    "    top10_words = diff_series.nlargest(10).index.tolist()\n",
    "    \n",
    "    # 计算这 10 个词的平均 Valence\n",
    "    # 直接从 df_clean 里查这些词的 valence_score 列\n",
    "    avg_val = df_clean.loc[top10_words, 'valence_score'].mean()\n",
    "    \n",
    "    # 格式化 Top 10 词汇为字符串 \"词1, 词2, ...\"\n",
    "    traits_str = \", \".join(top10_words)\n",
    "    \n",
    "    # 存入结果\n",
    "    results.append({\n",
    "        'Group A': p_a,\n",
    "        'vs. Group B': p_b,\n",
    "        'Top 10 Relative Traits': traits_str,\n",
    "        'Average Valence': round(avg_val, 3) # 保留3位小数\n",
    "    })\n",
    "\n",
    "# ================= 3. 保存结果 =================\n",
    "\n",
    "df_result = pd.DataFrame(results)\n",
    "\n",
    "# 按照你想要的格式列出\n",
    "# 比如你想看 北京 vs 其他所有省，可以按 Group A 排序\n",
    "df_result = df_result.sort_values(by=['Group A', 'vs. Group B'])\n",
    "\n",
    "print(df_result.head())\n",
    "df_result.to_csv(\"province_pairwise_comparison.csv\", encoding='utf-8-sig', index=False)\n",
    "print(\"计算完成，结果已保存至 province_pairwise_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c67a6a-0565-45e1-bfb2-2e234c3cba20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep_learn)",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
